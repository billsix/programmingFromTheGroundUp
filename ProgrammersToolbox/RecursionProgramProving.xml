<?xml version="1.0"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.1.2//EN">
<chapter>
<title>Recursion and Provably Correct Programs</title>

<para>
In this article we are going to tackle recursive programming patterns and how
they can be used to write provably correct programs.  Recursion is often given
a bad name because it is thought to be slow and wasteful of space.  However,
as we will see in this article, this need not be the case, and is not the
case in languages like Scheme.  Examples will be given in Scheme and C.
</para>

<sect1>
<title>Why use Recursion?</title>

<para>
For new computer science students, recursive programming is often difficult.
It is difficult to get your mind to think that way.  It almost seems like
circular reasoning.  When we give instructions to other humans, we rarely
direct them recursively.  Therefore, directing a computer recursively is
at first unintuitive.
</para>

<para>
For those of you who really are new to computer programming, here's what
recursion is -- recursion occurs when a function calls itself directly
or indirectly.  The classic example of recursive programming is computing
factorials.  The factorial of a number is that number times all of the
numbers below it until 1.  For example, <literal>factorial(5)</literal>
is the same as <literal>5*4*3*2*1</literal>.  <literal>factorial(3)</literal>
is <literal>3*2*1</literal>.  An interesting property of a factorial is that
the factorial of a number is equal to the starting number multiplied by 
the factorial of the number immediately below it.  <literal>factorial(5)</literal> is the same
as <literal>5 * factorial(4)</literal>.  You could almost write the 
factorial function as simply this:
</para>

<example>
<title>First try at factorial function</title>
<programlisting>
int factorial(int n)
{
	return n * factorial(n - 1);
}
</programlisting>
</example>

<para>
The problem with this is that it would run forever.  There is no place
where this function stops.  It would continually call <literal>factorial</literal>
until we hit the negative numbers, and then keep on going.  Therefore, we need a way to know
when to stop.  In the case of a factorial, factorials of numbers less
than one don't make any sense.  Therefore, we stop at the number one
and return the factorial of one (which is one).  Therefore, the real
factorial function will look like this:
</para>

<example>
<title>Actual factorial function</title>
<programlisting>
int factorial(int n)
{
	if(n == 1)
	{
		return 1;
	}
	else
	{
		return n * factorial(n - 1);
	}
}
</programlisting>
</example>

<para>
As you can see, as long as the initial value is above zero, this
function will terminate.  This stopping point is called the
<emphasis>base case</emphasis>.  A base case is the bottom point of a
recursive program where the operation is so trivial as to be able to
return an answer directly.  All recursive programs must have at least
one base case, and must guarantee that the program will hit one
eventually.  Otherwise the program would run forever, or until it ran
out of memory or stack space.
</para>

<!--
NOTE - REMOVING QUICKSORT BECAUSE IT IS TOO COMPLICATED
<para>
Let's look at one more recursive example - the quicksort.  In the 
quicksort, the program chooses from the data a <emphasis>pivot</emphasis>
value.  It then compares all of the elements in the list to the pivot, 
dividing it into two groups: those above and those below.  It then calls
quicksort on both sublists and attaches the results together.  Let's look
at pseudo-code for the quicksort:
</para>

<example>
<title>Quicksort pseudo-code</title>
<programlisting>
Algorithm Quicksort (list L)
	Is the list zero or one elements long?
		If yes, then return an empty list
	Otherwise:
		Choose element X from L at random
		Divide L into two lists:
			A - elements below X
			B - elements above X
		Join together the following:
			Quicksort(A) + X + Quicksort(B)
		Return the result
</programlisting>
</example>

<para>
This algorithm continually divides the list into pieces and does a partial
sort.  Note that in the join, if the left list is in order, the right
list is in order, and there are no overlapping elements, the result must 
be a sorted list.  Well, we verified that there were no overlapping elements
by sorting the list members based on their relationship to X.  Then, we
called Quicksort on the divided elements. Now, the interesting thing
about Quicksort is that it will continually divide the list until it is
either zero or one element large.  We know that a list of zero or one elements
is already sorted.  Therefore, we can guarantee that Quicksort always
returns a sorted list.
</para>
-->

<para>
Every recursive program follows the same basic pattern:
</para>

<orderedlist>
<listitem><para>Initialize the algorithm.   Recursive programs often need a seed value to start with.  This is done either by using a parameter passed to the function or by providing a gateway function which is non-recursive, but which sets up the seed values for the recursive calculation.</para></listitem>
<listitem><para>Check to see if the current value(s) being processed match the base case.  If so, process and return the value.</para></listitem>
<listitem><para>Redefine the answer in terms of a smaller or simpler subproblem or subproblems.</para></listitem>
<listitem><para>Run this algorithm on the subproblem.</para></listitem>
<listitem><para>Combine the results in the formulation of the answer.</para></listitem>
<listitem><para>Return the result.</para></listitem>
</orderedlist>

<para>
When writing recursive programs, sometimes finding the simpler subproblem can
be tricky.  However, when dealing with <emphasis>inductively-defined data sets</emphasis>,
it is considerably easier.  An inductively-defined data set is essentially a data structure
defined in terms of itself - this is called an inductive definition.  For example, linked lists are defined
in terms of themselves.  A linked list consists of a node structure that contains two members -
the data it is holding and a pointer to another node structure (or NULL, to terminate the list).
Because the node structure contains a pointer to a node structure within it, it is said to be
defined inductively.  Inductive data is fairly easy to write recursive procedures on.  Notice how, like our recursive 
programs, the definition of a linked list also contains a base case - the NULL pointer.  That
will work out great in order to provide a base case for most of our recursive functions.
</para>

<para>
Let's look at a few examples of recursive functions on linked lists.  Let's say we have
a list of numbers, and we want to sum them.  Let's go through each part of the recursive
pattern and identify how they apply to to our summation function:
</para>

<orderedlist>
<listitem><para>Initialized the algorithm - this algorithm's seed value is the first node to process, which is passed as a parameter.</para></listitem>
<listitem><para>Check for the base case - we need to check and see if the current node is the null list.  If so, we return 0, because the sum of all members of an empty list is zero.</para></listitem>
<listitem><para>Redefine the answer in terms of a simpler subproblem - we can define the answer as the sum of the rest of the list plus the contents of the current node.  To determine the sum of the rest of the list, we call this function again with the next node.</para></listitem>
<listitem><para>Combine the results - after the recursive call completes, we add the value of the current node to the results of the recursive call.</para></listitem>
</orderedlist>

<para>
Here is the pseudo-code and real code for the function:
</para>

<example>
<title>Pseudo-code for the sum_list program</title>
<programlisting>
function sum_list(list l)
	is l null?
		yes - the sum of an empty list is 0 - return that
	data = head of list l
	rest_of_list = rest of list l
	the sum of the list is:
		data + sum_list(rest_of_list)
</programlisting>
</example>

<para>
The pseudo-code matches almost identically with the Scheme method of doing it.
</para>

<example>
<title>Scheme code for the sum-list program</title>
<programlisting>
(define sum-list (lambda (l)
	(if (null? l)
		0
		(let (
				(data (car l))
				(rest-of-list (cdr l)))
			(+ data (sum-list rest-of-list))))))
</programlisting>
</example>

<para>
For this easy example, the C version is just as simple.
</para>

<example>
<title>C code for the sum_list program</title>
<programlisting>
int sum_list(struct list_node *l)
{
	if(l == NULL)
		return 0;

	return 	l.data + sum_list(l.next);
}
</programlisting>
</example>

<para>
You may be thinking that you know how to do it faster or better without recursion.
If so, just hold on, and we'll get to the hows and whys.  Let's look at two more examples
first.
</para>

<para>
Let's say that we have a list of strings and want to see if a certain string is contained
in that list.  The way to break this down into a simpler problem is to again look at the 
individual nodes.  The subproblem is "is the search string the same as the one in <emphasis>this
node</emphasis>?"  If so, you have your solution, and if not you are one step closer.  What's
the base case?  We have two: if the current node has the string, that's a base case (returning 
true), and if the list is empty, then that's a base case (returning false).  We may not hit
the first base case, but we are certain that if we don't hit the first one we will at least
hit the second one.
</para>

<example>
<title>Scheme code for determining if a given list contains a given string</title>
<programlisting>
(define is-in-list 
	(lambda (the-list the-string)
		;;Check for base case of "list empty"
		(if (null? the-list)
			#f
			;;Check for base case of "found item"
			(if (equal? the-string (car the-list))
				#t
				;;Run the algorithm on a smaller problem
				(is-in-list (cdr the-list) the-string)))))
</programlisting>
</example>

<para>
This recursive function is pretty good, but it has one main shortcoming -- every 
iteration of the recursion will be passing the <emphasis>same value</emphasis> for
<literal>the-string</literal>.  Passing the extra parameter can increase the overhead of the function call.  However, we can set up a closure at the beginning of the
function to keep this from happening:
</para>

<example>
<title>Scheme program for finding a string using a closure</title>
<programlisting>
(define is-in-list2
	(lambda (the-list the-string)
		(letrec ( 
				(recurse (lambda (internal-list)
						(if (null? internal-list)
							#f
							(if (equal? the-string (car internal-list))
								#t
								(recurse (cdr internal-list)))))))
			(recurse the-list))))
</programlisting>
</example>

<para>
This version of the program is a little harder to follow.  It defines a closure 
called <literal>recurse</literal> that can be called with only one parameter,
rather than two (for more information on closures, see this 
<ulink url="http://FIXME/">earlier article</ulink>).  We don't need to pass in <literal>the-string</literal>
because it is already in the parent environment and does not change from call to call.  Because <literal>recurse</literal>
is defined <emphasis>within</emphasis> the <literal>is-in-list2</literal> function, it can see all of the
currently-defined variables, so they don't need to be re-passed.  This shaves off one variable being
passed at each iteration.  This doesn't make a lot of difference in this trivial example, but it can
save a lot of typing, a lot of errors, and a lot of overhead of passing variables in more complex 
functions.
</para>

<para>
Making recursive closures in this way is a bit tedious.  This same pattern of creating a recursive closure
using <literal>letrec</literal> and then calling it with an initial seed value occurs over and over again
in recursive programming.  Therefore, in Scheme there is a shortcut called the <emphasis>named let</emphasis>.
This construct looks a lot like a let, except that the whole block is named so that it can be called as
a recursive closure.  The parameters of the function and the initial seed values are defined like a normal
let, and each successive recursive call uses the parameters as new values.  It's fairly confusing to talk
about, so take a look at the code, and compare it to the code above.
<!-- FIXME - I probably need to see how other people define named let's, because I'm not sure mine works well -->
</para>

<example>
<title>Named let example</title>
<programlisting>
(define is-in-list2
	(lambda (the-list the-string)
		;;Named Let
		;;This let block defines a function called "recurse" that is th
		;;body of this let.  The function's parameters are the same as
		;;the variables listed in the let.
		(let recurse
			;;internal-list is the first and only parameter.  The
			;;first time through the block it will be primed with
			;;"the-list" and subsequent calls to "recurse" will
			;;give it whatever value is passed to "recurse"
			( (internal-list the-list) )
			;;Body of function/named let block
			(if (null? internal-list)
				#f
				(if (equal? the-string (car internal-list))
					#t
					;;Call recursive function with the
					;;rest of the list
					(recurse (cdr internal-list)))))))
</programlisting>
</example>


<para>
This cuts down considerably on the amount of typing and mistakes made when writing
recursive functions.  If you are still having trouble with the concept of named lets,
I suggest that you thoroughly compare every line in the above two programs, as well
as look at some of the documents in the <ulink url="#resources">Resources</ulink>
section of this article.
</para>

<para>
Our next example of a recursive function on lists will be a little more complicated.
It will check to see whether or not a list is in ascending order.  If it is, it 
will return <literal>#t</literal>, otherwise it will return <literal>#f</literal>.
This program will be a little different because in addition to having to examine the
current value, we will also have to remember the last value processed.
</para>

<para>
The first item on the list will have to be processed special, because it won't have
an item preceding it.  For the remaining items, we will need to pass the previously
examined data item in the function call.  The function looks like this:
</para>

<example>
<title>Scheme program to determine if a list is in ascending order</title>
<programlisting>
(define is-ascending
	(lambda (the-list)
		;;First, Initialize the algorithm.  To do this we
		;;need to get the first value, if it exists, and
		;;use it as a seed to the recursive function
		(if (null? the-list)
			#t
			(let is-ascending-recurse
				( 
					(previous-item (car the-list))
					(remaining-items (cdr the-list))
				)
				;;Base case #1 - end of list
				(if (null? remaining-items)
					#t
					(if (< previous-item (car remaining-items))
						;;Recursive case, check the rest of the list
						(is-ascending-recurse (car remaining-items) (cdr remaining-items))
						;;Base case #2 - not in ascending order
						#f))))))
</programlisting>
</example>

<para>
This program begins by first checking a boundary condition -- whether or not the list is empty.
It then seeds the recursive function with the first item on the list, and the remaining list.
The base case is checked.  The only way to get to the end of the list is if everything so far
has been in order.  Therefore, if the list is empty, the list is in ascending order.  Otherwise,
we check the current item.  If the current item is in ascending order, we then only have a subset
of the problem left to solve -- whether or not the rest of the list is in ascending order.  So
we recurse with the rest of the list and try it again.
</para>

<para>
Notice in this function how we maintained state through function calls by passing it forward.
Previously we had just passed the remainder of the list each time.  In this function, though, 
we needed to know a little bit more about the state of the computation.  The result of the
present computation depended on the partial results before it.  Therefore, in each successive
recursive call, we pass those results forward.  This is a common pattern for more complex
recursive procedures.
</para>

</sect1>

<sect1>
<title>Provably Correct Programs</title>

<para>
Bugs are a part of the daily life of every programmer.  Even the smallest loops, or the
tiniest function call can have bugs in it.  What's more, while most programmers can 
examine code and test code for bugs, they do not know how to prove that their programs
perform the way they think they will.
</para>

<para>
One of the primary sources of bugs in computer programs is variables changing states. 
You might think that the programmer would be keenly aware of exactly how and
when a variable changes state.  This is sometimes true in simple loops, but usually not in complex ones.  Usually within loops there are several ways
that a given variable can change state.  One of several if statements may make modifications.
The order is usually important.  However, being absolutely sure that you proceeded in the correct order for all cases is difficult.  It is even more difficult to fix bugs in such loops without causing problems for other cases.
</para>

<para>
In order to alleviate the situation, we need to be able to:
</para>

<itemizedlist>
<listitem><para>Be able to tell by sight the derivation of each variable.</para></listitem>
<listitem><para>Be certain that no variable is performing double-duty (many programmers often use the same variable to store two related, but slightly different values).</para></listitem>
<listitem><para>Be certain that all variables hit the state they are supposed to be in when the loop restarts (a common programming error is to not set new values for loop variables in corner cases that are rarely used and tested).</para></listitem>
</itemizedlist>

<para>
To accomplish these goals, we only need to make one rule in our programming -- <emphasis>only assign a value to a variable
once, and NEVER MODIFY IT</emphasis>.  This is blasphemy for many who are raised on imperative and object-oriented
programming -- variable assignment and modification are at the core of these programming techniques!  Yet, state changes
are consistently one of the chief programming errors for imperative programmers.
</para>

<para>
So how does a person program without modifying variables?  Let's look at several situations where we often modify
variables and see how we can get by without doing so.
</para>

<variablelist>
<varlistentry>
<term>Reusing a variable</term>
<listitem><para>Often times a variable is reused for different, but similar purposes.  <!-- FIXME - need example -->
To alleviate this problem, create two separate variables, and just derive the second from the first the same way 
you would do so if you were writing to the same variable.</para></listitem>
</varlistentry>

<varlistentry>
<term>Conditional Modification of a Variable</term>
<listitem><para>
This is a subset of the "reusing a variable" problem.
However, in this case, sometimes we will keep our existing value, and
sometimes we will want a new value.  Well, again, we create a new
variable.  In most languages, we can use the tertiary operator
<literal>? :</literal> to set the value of the new variable.  For
example, if we wanted to give our new variable a new value as long as
it's not greater than <literal>some_value</literal> we could do
<literal>int new_variable = old_variable > some_value ? old variable : new_value;</literal>.  
</para></listitem>
</varlistentry>

<varlistentry>
<term>Looping Variables</term>
<listitem><para>
We will show how to get rid of these later.
</para></listitem>
</varlistentry>

</variablelist>

<para>
Once we have rid ourselves of all variable state changes, we can know that when we first define
our function, the definition of our function will hold for as long as the variable lasts.  This 
makes sequencing orders of operations much easier, especially when modifying existing code.
You don't have to worry about what sequence a variable might have been modified in, and what 
assumptions were being made about its state at each juncture.  Since a variable cannot change
state, the full definition of how it is derived is shown right where it is declared!  You never
have to go searching through code to find the incorrect or misordered state change again!  
</para>

<para>
Now, the question is, how do we do loops without assignment?  The answer is 
<emphasis>recursive functions</emphasis>.  This may be surprising, but let's look at the properties of loops and see how
they compare to recursive functions:
</para>

<variablelist>

<varlistentry>
<term>Repetition</term>
<listitem><para>
Loops execute the same block of code repeatedly to obtain the result.  Likewise, recursive functions
execute the same block of code repeatedly to obtain the result.  The difference is that loops signal their intent to 
repeat by either finishing the block of code or issuing a <literal>continue</literal> command, while
recursive functions repeat by calling themselves.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Terminating Conditions</term>
<listitem><para>
In order to guarantee that it will terminate, a loop must have one or more conditions which cause
it to terminate, and it must be guaranteed to hit one.  Likewise, recursive functions require a
base case, which causes the function to stop recursing. 
</para></listitem>
</varlistentry>

<varlistentry>
<term>State</term>
<listitem><para>
Loops have a current state which is updated as the loop progresses.  Recursive functions have a 
current state which is passed as parameters.
</para></listitem>
</varlistentry>

</variablelist>

<para>
As you can see, recursive functions and loops have quite a bit in common.  In fact, loops and
recursive functions are interchangeable.  The difference is that with recursive functions, we
can manage to never (well, rarely) have to modify any variable, we just pass the new values as
parameters to the next function call.  This allows us to keep all of the benefits of not having
an updateable variable, while still having repetitive, stateful behavior.
</para>

<para>
Let's take a look at a common loop for printing reports and see how it can convert into a recursive function.
This loop will perform the following operations:
</para>

<itemizedlist>
<listitem><para>Print out the page number and page headers at each page break.</para></listitem>
<listitem><para>We'll assume that the report lines are grouped by some numeric criteria.  We'll pretend there is some total we are keeping track of for these groups.</para></listitem>
<listitem><para>At the end of each grouping, we'll print out the totals for that group.</para></listitem>
</itemizedlist>

<para>
For demonstration purposes, we've left out all of the subordinate functions.  We'll just assume they exist
and assume they perform as expected.  Here is the code for our report printer.
</para>

<example>
<title>Report-printing program using a normal loop</title>
<programlisting>
void print_report(struct report_line *report_lines, int num_lines)
{
	int num_lines_this_page = 0;
	int page_number = 1;
	int current_line; /* iterates through the lines */
	int current_group = 0; /* tells which grouping we are in */
	int previous_group = 0; /* tells which grouping was here on the last loop */
	int group_total = 0; /* saves totals for printout at the end of the grouping */

	print_headings(page_number);

	for(current_line = 0; current_line < num_lines; current_line++)
	{
		num_lines_this_page++;

		if(num_lines_this_page == LINES_PER_PAGE)
		{
			page_number++;
			page_break();
			print_headings(page_number);
		}

		current_group = get_group(report_lines[current_line]);
		if(current_group != previous_group)
		{
			print_totals_for_group(group_total);
			group_total = 0;
		}

		print_line(report_lines[current_line]);
		group_total += get_line_amount(report_lines[current_line]);
	}
}

</programlisting>
</example>

<para>
I've intentionally left several bugs in the program.  Try and see if you can spot them.  Because we
are continually modifying state variables, it is difficult to see whether or not at any given moment
they are correct.  Here is the same program done recursively:
</para>

<example>
<title>Report-printing program using recursion</title>
<programlisting>
void print_report(struct report_line *report_lines, int num_lines)
{
	int num_lines_this_page = 0;
	int page_number = 1;
	int current_line; /* iterates through the lines */
	int current_group = 0; /* tells which grouping we are in */
	int previous_group = 0; /* tells which grouping was here on the last loop */
	int group_total = 0; /* saves totals for printout at the end of the grouping */


	/* initialize */	
	print_headings(page_number);

	/* Seed the values */
	print_report_i(report_lines, 0, 1, 1, 0, 0, num_lines);
}

void print_report_i(struct report_line *report_lines, /* our structure */
	int current_line, /* current index into structure */
	int num_lines_this_page, /* number of lines we've filled this page */
	int page_number, 
	int previous_group, /* used to know when to print totals */
	int group_total, /* current aggregated total */
	int num_lines) /* the total number of lines in the structure */
{
	if(current_line == num_lines)
	{
		return;
	}
	else
	{
		if(num_lines_this_page == LINES_PER_PAGE)
		{
			page_break();
			print_headings(page_number + 1);
			print_report_i(
				report_lines, 
				current_line, 
				1, 
				page_number + 1, 
				previous_group, 
				group_total, 
				num_lines);
		}
		else
		{
			int current_group = get_group(report_lines[current_line]);
			if(current_group != previous_group && previous_group != 0)
			{
				print_totals_for_group(group_total);
				print_report_i(
					report_lines, 
					current_line, 
					num_lines_this_page + 1, 
					page_number, 
					current_group, 
					0, 
					num_lines);
			}
			else
			{
				print_line(report_lines[current_line]);
				print_report_i(
					report_lines, 
					current_line + 1, 
					num_lines_this_page + 1, 
					page_number, 
					current_group, 
					group_total + get_line_amount(report_lines[current_line]), 
					num_lines);
			}
		}
	}
}
</programlisting>
</example>

<para>
If you notice, there is never a time when the numbers we are using is not
self-consistent.  Almost anytime you have multiple states changing, you 
will have several lines during the state change where the program will
not have self-consistent numbers.  If you then add a line to your program
in the middle of such state changes, it is difficult to tell whether your
conception of the states of the variables match reality.  After several 
modifications, it is likely that subtle bugs will be introduced.  In this
program, all state changes are brought about by re-running the recursive
function with completely self-consistent data.  
</para>

<para>
Because you never change the states of your variables, program proving is much easier. Let's look at a proof for the above program.  Since this program
has several features, we will show short proofs for each of them.  There are three different places where we recurse, and they will be referred to as R1, R2, and R3, respectively.
</para>

<variablelist>

<varlistentry>
<term>Program Termination</term>
<listitem><para>Assumptions: <literal>num_lines</literal> &gt;= <literal>current_line</literal> and <literal>LINES_PER_PAGE</literal> &gt; 1.  The program stops when current_line == num_lines.  current_line either increments by 1 (R3) or stays the same (R1 and R2).  R2 will only occur when the current value of current_line is different than the previous value of current_line, because current_group and previous_group are directly derived from those. R1 can only occur by changes in num_lines_this_page, which can only result from R2 and R3.  Since R2 can only occur on the basis of R3 and R1 can only occur on the basis of R2 and R3, we can conclude that current_line is increasing.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Page breaks occur after LINES_PER_PAGE lines</term>
<listitem><para>
Assumptions: The current page already has a page header printed on the first line.  Page breaks occur every time num_lines_this_page is equal to LINES_PER_PAGE by inspection.  Page breaks always print new headings.  New headings set the num_lines_this_page to 1 (R1).  num_lines_per_page is incremented immediately after printing group totals (R2) and report lines (R3).  Therefore every print event modifies num_lines_this_page.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Every report item line is printed exactly once</term>
<listitem><para>
A report item line is not printed in conjunction R1 or R2, but only with R3.  R3 increases current_line by one, while R1 and R2 preserve current_line.  current_line.  We have proven that the program terminates, and this occurs when current_line == num_lines.  Therefore, we know that each line is printed exactly once.
</para></listitem>
</varlistentry>

</variablelist>

<para>
These are just some of the proofs that are available.  They can be done much
more rigorously, but many of us program because we can't stand the tedium of
mathematics nor its notation.
</para>

<para> 
Don't get me wrong -- it's not that program proofs cannot be
done with imperative programs, it's that the number of state changes
that occur make them unwieldy.  With recursive programs that recursive
instead of change state, the number of occasions of state change is
small, and they maintain self-consistency.  This does not outright
prevent logical errors, but it does eliminate numerous classes of
them.  This method of programming is usually termed functional
programming.
</para>

</sect1>

<sect1 id="loopstorecursion">
<title>Tail-Recursive Functions</title>

<para>
So we've seen how loops and recursive functions are related.  We've seen 
how we can convert our loops into recursive, non-state-changing functions 
and achieve a result that is more maintainable and provably correct than
what we started with.  However, many people worry about the growth of
stack space using recursive functions.  This is true for some classes of
recursive functions.  However, for <emphasis>tail-recursive</emphasis>
functions, which we will look at in this section, stack size remains
constant.
</para>

<sect2>
<title>Tail Recursion</title>

<para>
When we converted our loop to a recursive function, the recursive call
was the last thing that the function did.  If you evaluate <literal>print_report_i</literal>, you will see that there is nothing further that happens in the function after the recursive call.  This is because of its loop-like behavior -- when loops hit the end of the loop or if they issue a <literal>continue</literal>, then that is the last thing they do in that block of code.
</para>

<para>
A function call (recursive or not) that is the last thing a function does is called a <emphasis>tail-call</emphasis>.  Recursion using tail-calls is called <emphasis>tail-recursion</emphasis>.  Let's look at some example function calls to see exactly what is meant by a tail-call:
</para>

<example>
<title>Tail-calls and non-tail calls</title>
<programlisting>
int test1()
{
	int a = 3;
	test1(); /* recursive, but not a tail call */
	a = a + 4;
	return a;
}

int test2()
{
	int q = 4;;
	q = q + 5;
	return q + test1(); /* test1() is not in tail position.
	                     * There is still more work to be
	                     * done after test1() returns (like
	                     * adding q to the result
	                     */
}

int test3()
{
	int b = 5;

	b = b + 2;

	return test1();  /* This is a tail-call.  The return value
	                  * of test1() is used as the return value
	                  * for this function.
	                  */
}

int test4()
{
	test3(); /* not in tail position */
	test3(); /* not in tail position */
	return test3(); /* in tail position */
}
</programlisting>
</example>

<para>
As you can see, <emphasis>no other operation</emphasis> can be performed on 
the result of the tail-called function before it is passed back in order
for the call to truly be a tail-call.
</para>

<para>
Notice that since there is nothing left to do in the function, the actual stack frame for the function is not needed, either.  The only
issue is that most programming languages don't know how to get rid of unused
stack frames, especially in terms of finding the return address.  If we were
to be able to find a way to remove these unneeded stack frames, our 
tail-recursive functions would run in a constant stack size.
</para>

</sect2>

<sect2>
<title>Tail-call Optimization</title>

<para>
The method of removing stack frames after tail-calls is called <emphasis>tail-call optimization</emphasis>.
So what is the optimization?  Ask yourself this question -- after the function
in tail position is called, which of our local variables will be in use?
None.  What processing will be done to the return value?  None.  Which 
parameters passed to the function will be used?  None.  Interestingly, it seems
that once control is passed to the tail-called function, nothing in the stack
is useful anymore.  The function's stack frame, while it still takes up space,
is actually at this point useless.  Therefore, the tail-call optimization
is to <emphasis>overwrite</emphasis> the current stack frame with the next
one when making a function call in tail position.
</para>

<para>
Essentially what we are doing is surgery on the stack.  The activation
record isn't needed anymore, so we are going to cut it out, and redirect
the tail-called function back to the function that called us.  This means
that we have to manually rewrite the stack to fake a return address so that
the tail-called function will return directly to our parent. 
</para>

<!-- <remark>Graphic here representing this</remark> -->

<para>
For those of you who like to actually mess with the low-level stuff, here
is an assembly language template for an optimized tail-call:
</para>

<example>
<title>Assembly language template for tail-calls</title>
<programlisting>
;;Unoptimized tail-call

my_function:
	...
	...

	;PUSH ARGUMENTS FOR the_function HERE

	call the_function

	;results are already in %eax so we can just return
	movl %ebp, %esp
	popl %ebp
	ret

;;Optimized tail-call
optimized_function:
	...
	...

	;save the old return address
	movl 4(%ebp), %eax

	;save old %ebp
	movl (%ebp), %ebx 

	;Clear stack activation record (assuming no unknowns like 
	;variable-size argument lists)
	addl $(SIZE_OF_PARAMETERS + 8), %ebp ;(8 is old %ebp + return address))

	;restore the stack to where it was before the function call
	movl %ebp, %esp

	;Push arguments onto the stack here

	;push return address
	pushl %ebx 

	;Execute the function
	jmp the_function
</programlisting>
</example>

<para>
As you can see, tail-calls take a few more instructions,
but they can save quite a bit of memory.  There are a few restrictions
when using them, however:
</para>

<itemizedlist>
<listitem><para>The calling function must not depend on the parameters list
still being on the stack when your function returns to it.</para></listitem>
<listitem><para>The calling function must not care where the stack pointer is currently pointing (of course, it can assume that it is past its local variables).  This means that you cannot compile using <literal>-fomit-frame-pointer</literal></para></listitem>
<listitem><para>There can be no variable-length argument lists</para></listitem>
</itemizedlist>

<para>
When a function calls itself in a tail-call, the method is even easier.
You simply move the new parameters on top of the old ones and do a jump to
the point right after local variables are saved on the stack.  Since we are
just jumping into the same function, the return address and old 
<literal>%ebp</literal> will be the same and the stack size won't change.  
Therefore, the only thing we need to do before the jump is replace the 
old parameters with the new ones.
</para>

<para>
So, for the price of a few instructions, your program can have the provability
of a functional program and the speed and memory characteristics of an
imperative one.  The only problem is that right now very few compilers
implement tail-call optimizations.  Scheme implementations are required
to implement them, and many other functional language implementations do so,
too.  Note, however, that because functional languages sometimes use the stack
much differently than imperative languages (or not use the stack at all) their methods of
implementing tail-call optimizations can be quite different.
</para>

</sect2>

</sect1>
	
</chapter>
