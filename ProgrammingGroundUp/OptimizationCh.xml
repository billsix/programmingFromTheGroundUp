<chapter>
<title>Optimization</title>
<!--

Copyright 2002 Jonathan Bartlett

Permission is granted to copy, distribute and/or modify this
document under the terms of the GNU Free Documentation License,
Version 1.1 or any later version published by the Free Software
Foundation; with no Invariant Sections, with no Front-Cover Texts,
and with no Back-Cover Texts.  A copy of the license is included in fdl.xml

-->

<para>
Optimization is the process of making your application run more
effectively.  You can optimize for many things - speed, memory
space usage, disk space usage, etc. - however, this chapter 
focuses on speed optimization.
</para>

<sect1>
<title>When to Optimize</title>

<para>
It is better to not optimize than to optimize too soon.  When you
optimize, your code generally becomes less clear, because it becomes
more complex.  Readers of your code will have more trouble discovering
why you did what you did which will increase the cost of maintenance 
of your project.  Even knowing how and why your program runs the way
it does, optimized code is harder to debug and extend.  It slows the
development process down considerably, both because of the time it
takes to optimize the code, and the time it takes to modify your 
optimized code.
</para>

<para>
Compounding this problem is that you don't even know where
the speed issues in your program will be.  Even experienced programmers
have trouble predicting which parts of the program will need optimization,
so you will probably end up wasting your time optimizing the wrong parts.
<xref linkend="wheretooptimize" /> will discuss how to find the parts of
your program that need optimization.
</para>

<para>
While you develop your program, you need to have the following priorities:

<itemizedlist>
<listitem><para>Everything is documented</para></listitem>
<listitem><para>Everything works as documented</para></listitem>
<listitem><para>The code is written in an easily modifiable form</para></listitem>
</itemizedlist>

Documentation is essential, especially when working in groups.  The proper
functioning of the program is essential.  See 
<xref linkend="developingrobustprograms" /> for why it is best to fix 
problems before adding new features or optimizations.  You'll notice application
speed was not anywhere on that list.  Optimization is
not necessary during early development for the following reasons:

<itemizedlist>
<listitem><para>Minor speed problems can be usually solved through hardware, which is often much cheaper than your time</para></listitem>
<listitem><para>Your application will change dramatically as you revise it, therefore wasting most of your efforts to optimize it<footnote><para>Many
new projects often have a first code base which is completely rewritten
as developers learn more about the problem they are trying to solve.  Any
optimization done on the first codebase is completely wasted.</para></footnote>
</para></listitem>
<listitem><para>Speed problems are usually localized in a few places in your code - finding these is difficult before you have most of the program in place.</para></listitem>
</itemizedlist>
</para>

<para>
Therefore, the time to optimize is toward the end of development, when you
have determined that you actually have performance problems. 
</para>

<remark>Include my story about optimize the Wolfram Web Store?</remark>

</sect1>

<sect1 id="wheretooptimize">
<title>Where to Optimize</title>

<para>
Once you have determined that you have a performance issue you need
to determine where in the code the problems occur.  You can do this
by running a 
<emphasis>profiler</emphasis><indexterm><primary>Profiling</primary></indexterm>.
A profiler is a program that will let you run your program, and it
will tell you how much time is spent in each function, and how many
times they are run.  A discussion of using profilers is outside 
the scope of this text.  The functions that are called the most or
have the most time spent in them are the ones that should be optimized.
</para>

<para>
In order to optimize functions, you need to understand in what ways they
are being called and used.  The more you know about how and when a function
is called, the better position you will be in to optimize it appropriately.
</para>

<para>
There are two main categories of optimization - local optimizations and
global optimizations.  Local optimizations consist of optimizations that are
either hardware specific - such as the fastest way to perform a given
computation - or program-specific - such as making a specific piece of
code perform the best for the most often-occuring case.  Global optimization
consist of optimizations which are structural.  For example, if you were
trying to find the best way for three people in different cities to meet in
St. Louis, a local optimization would be finding a better road to get there,
while a global optimization would be to decide to teleconference instead of
meeting in person.  Global optimization often involves restructuring code
to avoid performance problems, rather than trying to find the best way through them.
</para>

</sect1>

<sect1>
<title>Local Optimizations</title>

<para>
The following are some well-known methods of optimizing pieces of code.  When
using high level languages, some of these may be done automatically by your
compiler's optimizer.
</para>

<variablelist>

<varlistentry>
<term>Precomputing Calculations</term>
<listitem><para>
Sometimes a function has a limitted number of possible inputs and outputs.  In
fact, it may be so few that you can actually precompute all of the possible
answers beforehand, and simply look up the answer when the function is called.
This takes up some space since you have to store all of the answers, but for
small sets of data this works out really well, especially if the computation
normally takes a long time.  
This only works with true, stateless functions that use simple data.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Remembering Calculation Results</term>
<listitem><para>
This is similar to the previous method, but instead of computing results
beforehand, the result of each calculation requested is stored.  This way
when the function starts, if the result has been computed before it will
simply return the previous answer, otherwise it will do the full computation
and store the result for later lookup.  This has the advantage of requiring
less storage space because you aren't precomputing all results.  This
is sometimes termed <emphasis>caching</emphasis> or <emphasis>memoizing</emphasis>.
This only works with true, stateless functions that use simple data.
</para></listitem>
</varlistentry> 

<varlistentry>
<term>Locality of Reference</term>
<listitem><para>
<emphasis>Locality of reference</emphasis> is a term for where in memory the 
data items you are accessing are.  With virtual memory, you may access pages
of memory which are stored on disk.  In such a case, the operating system has
to load that memory page from disk, and unload others to disk.  Let's say, 
for instance, that the operating system will allow you to have 20k of memory
in physical memory and forces the rest of it to be on disk, and your 
application uses 60k of memory.  Let's say your program has to do 5 operations 
on each piece of data.  If it does one operation on every piece of data, and 
then goes through and does the next operation on each piece of data, eventually
every page of data will be loaded and unloaded from the disk 5 times.  Instead,
if you did all 5 operations on a given data item, you only have to load each
page from disk once.  When you bundle as many operations on data that is
physically close to each other in memory, then you are taking advantage of 
locality of reference.  In addition, processors usually store some data on-chip
in a cache.  If you keep all of your operations within a small area of physical
memory, you can bypass even main memory and only use the chips ultra-fast cache
memory.  This is all done for you - all you have to do is to try to operate on
small sections of memory at a time, rather than bouncing all over the place in
memory.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Register Usage</term>
<listitem><para>
Registers are the fastest memory locations on the computer.  When you access
memory, the processor has to wait while it is loaded from the memory bus.  However,
registers are located on the processor itself, so access is extremely fast.  Therefore
making wise usage of registers is extremely important.  If you have few enough data
items you are working with, try to store them all in registers.  In high level languages,
you do not always have this option - the compiler decides what goes in registers and
what doesn't.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Inline Functions</term>
<listitem><para>
Functions are great from the point of view of program management - they make it easy
to break up your program into independent, understandable, and reuseable parts.  However,
function calls do involve the overhead of pushing arguments onto the stack and doing the
jump (remember locality of reference - your code may be on disk instead of in memory)s.  It's 
also impossible for compilers to do optimizations across function-call boundaries.  However,
some languages support inline functions.  These functions look, smell, taste, and act like
real functions, except the compiler has the option to simply plug the code in exactly where
it was called.  This makes the program faster, but it also increases the size of the code.
There are also many functions, like recursive functions, which cannot be inlined because
they call themselves either directly or indirectly.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Optimized Instructions</term>
<listitem><para>
Often times there are multiple assembly language instructions which accomplish the same purpose.  
A skilled assembly language programmer knows which instructions are the fastest.  However,
this can change from processor to processor.  For more information on this topic, you need to 
see the user's manual that is provided for the specific chip you are using.  An example of this
that is true on most processors is loading a 0 into a register.  On most processors, doing a
<command>movl $0, %eax</command> is not the quickest way.  The quickest way is to exclusive-or
the register with itself, <command>xorl %eax, %eax</command>.  This is because it only has
to access the register, and doesn't have to transfer any data.  For users of high-level languages,
the compiler handles this kind of optimizations for you.  For assembly-language programmers, you
need to know your processor well.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Addressing Modes</term>
<listitem><para>
Different addressing modes work at different speeds.  The fastest are the immediate and register
addressing modes.  Direct is the next fastest, indirect is next, and base-pointer and indexed
indirect are the slowest.  Try to use the faster addressing modes, when possible.  One interesting
consequence of this is that when you have a structured piece of memory that you are accessing using 
base-pointer addressing, the first element can be accessed the quickest.  Since it's offset is 0,
you can access it using indirect addressing instead of base-pointer addressing, which makes it 
faster.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Data Alignment</term>
<listitem><para>
Some processors can access data on word-aligned memory boundaries (i.e. - addresses divisible by
the word size) faster than non-aligned data.  So, when setting up structures in memory, it is 
best to keep it word-aligned.  Some processors, in fact, cannot access non-aligned data in some
modes.
</para></listitem>
</varlistentry>

</variablelist>

<para>
These are just a smattering of examples of the kinds of local optimizations possible.  However,
remember that the maintainability and readability of code is much more important except under
extreme circumstances.
</para>

</sect1>

<sect1>
<title>Global Optimization</title>

<para>
One of the goals of global optimizations is to put your code
in a form where it is easy to do local optimiztions.  For
example, if you have a large procedure that performs several
slow, complex calculations, you might see if you can break
parts of that procedure into their own functions where the
values can be precomputed or memoized. 
</para>

<para>
Stateless behavior is the easiest type of behavior to optimize
in a computer.  The more stateless parts of your program you
have, the more opportunities you have to optimize.  In one
program I wrote, the computer had to find all of the associated
parts for specific inventory items.  This required about 12
database calls, and took about 20 seconds.  However, the goal
of this program was to be interactive, and a 20-second wait
does not qualify as that.  However, I knew that these inventory
configurations do not change.  Therefore, I converted the 
database calls into their own functions, which were stateless.
I was then able to memoize the functions.  At the beginning of
each day, the function results were cleared in case anyone had
changed them, and several inventory items were automatically
preloaded.  From then on during the day, the first time someone
accessed an inventory item, it would take the 20 seconds it
did beforehand, but afterwards it would take less than a second,
because the database results had been memoized.
</para>

<para>
Global optimization usually often involves achieving the following 
properties in your functions:

<variablelist>

<varlistentry>
<term>Parallelization</term>
<listitem><para>
Parallelization means that your algorithm
can effectively be split among multiple processes.  For example, 
pregnancy is not very parallelizable because no matter how many women
you have, it still takes nine months.  However, building a car is
parallelizable because you can have one worker working on the engine
while another one is working on the interior.  Usually, applications
have a limit to how parallelizable they are.  The more parallelizable
your application is, the better it can take advantage of multiprocessor
and clustered computers.
</para></listitem>
</varlistentry>

<varlistentry>
<term>Statelessness</term>
<listitem><para>
Stateless functions and programs are those that rely entirely on the
data explicitly passed to them for functioning.  For example, mathematical
functions are stateless.  The multiplication function will always produce
the same output given the same inputs.  However, sending in an order form
to a company is not stateless.  If they run out of supplies, they will not
be able to fulfill your order.  If they increase the price of the item you
are purchasing, they will reject your order as well.  Therefore, the output
that results from you submitting your order changes based on their state
of affairs.  Most things are not entirely stateless, but they can be within
limits.  In my inventory program example, the function wasn't entirely stateless,
but it was within the confines of a single day.  Therefore, I optimized it
as if it were a stateless function, but made allowances for changes at night.
Two great benefits resulting from statelessness is that most stateless functions
are also parallelizable, and stateless functions can often benefit from memoization.
</para></listitem>
</varlistentry>

</variablelist>

</para>

<para>
Global optimization takes quite a bit of practice to know what works and
what doesn't.  Deciding how to tackle optimization problems in code involves
looking at all the issues, and knowing that fixing some issues may cause others.
</para>

</sect1>

<!--FIXME - add a section about how changing assumptions can affect speed -->

<sect1>
<title>Projects</title>

<itemizedlist>
<listitem><para>Go back through each program in this book and try to make optimizations according to the procedures outlined in this chapter</para></listitem>
<listitem><para>Pick a program from the previous exercize and try to calculate the performance impact on your code under specific inputs.<footnote><para>Since these programs are usually short enough not to have performance problems, looping through the program thousands of times will exaggerate the time it takes to run enough to make calculations.</para></footnote></para></listitem>
<listitem><para>Think of a program that you find particularly slow, and try to imagine the reasons for the slowness.  Try to think of ways that these could be improved.  If the program you are thinking of is an open-source project, go and evaluate both your guessed reasons and your proposed solution based on the code itself.</para></listitem>
</itemizedlist>

</sect1>

</chapter>
